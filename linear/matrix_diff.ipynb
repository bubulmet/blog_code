{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# про матричное дифференциорвание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ml-handbook.ru/chapters/matrix_diff/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:28:50.110706Z",
     "start_time": "2022-05-27T07:28:50.106517Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:28:51.453726Z",
     "start_time": "2022-05-27T07:28:50.536201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:28:51.472131Z",
     "start_time": "2022-05-27T07:28:51.468130Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (17,8)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## скалярное умножение и trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор по умолчанию - столбец. У него есть строки, но нет столбцов (ну, он один). Ведь первое измерение - это кол-во строк, а потом уже идет количество столбцов.  \n",
    "\n",
    "Поэтому скалярное умножение векторов $<\\vec{a}, \\vec{b}>$ записывется как $a^T b$: мы транспонируем $\\vec{a}$, и умножаем строку на столбец. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(A)$ - это **trace** квадратной матрицы $A$. По определению - это сумма элементов на главной диагонали.  \n",
    "\n",
    "Нам интересно вот какое свойство trace'а квадратной матрицы, получаемой перемножением матриц:  \n",
    "$$tr(A^TB) = tr(AB^T) = tr(B^TA) = tr(BA^T) = \\sum_{i=1}^{m}\\sum_{j=1}^{n}a_{ij}b_{ij}$$  \n",
    "\n",
    "Т.е. это сумма поэлементного перемножения матриц!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:42:59.300258Z",
     "start_time": "2022-05-27T07:42:59.280028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3,1,2,3,1,2,3,1,2,3]).reshape(4,3)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:43:03.188209Z",
     "start_time": "2022-05-27T07:43:03.175271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([4,5,6,4,5,6,4,5,6,4,5,6]).reshape(4,3)\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:43:11.487804Z",
     "start_time": "2022-05-27T07:43:11.469089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 20, 24],\n",
       "       [32, 40, 48],\n",
       "       [48, 60, 72]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.dot(A.T, B)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T07:43:45.481539Z",
     "start_time": "2022-05-27T07:43:45.468546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(C), (A * B).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или лучше так сказать: $tr(A^TB)$ - это аналог $< \\vec a , \\vec b >$ (скалярного умножения векторов, одинаковой размерности конечно) для случая матриц (также одинаковой размерности): поэлементное умножение и суммирование результатов. В одномерном же случае это просто умножение: $a \\cdot b $.  \n",
    "\n",
    "Так вот, в таком вот расширительном толковании можно говорить, что, какова бы ни была размерность $x$ и $h$ (но она у них, конечно, совпадает), ***дифференциал скалярной функции $f(x)$ от приращения $h$ в точке $x_0$ равен $< \\nabla f_{x_0} , h >$***  \n",
    "\n",
    "Тут мы, получается, несколько расширяем понятие градиента, относя к нему и просто значение производной (этакий одномерный градиент) и \"двумерный градиент\", матрицу из частных производных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним базу матана:  \n",
    "$$f(x_0 + h) \\approx f(x_0) + [D_{x_0}f](h)$$  \n",
    "\n",
    "Здесь $[D_{x_0}f]$ - **дифференциал** функции $f(x)$ в точке $x_0$. Он позволяет нам примерно предсказать значение $f(x)$ в случае малого приращения $h$.  \n",
    "\n",
    "Так вот, прикол в следующем: для скалярных функций (т.е. которые на выходе дают скаляр) верно следующее:  \n",
    "$$[D_{x_0}f](h) = \\langle \\nabla f_{x_0} , h \\rangle$$  \n",
    "\n",
    "Здесь мы трактуем градиент и скалярное умножение в этом самом широком смысле: это может быть одно число (значение производной в точке $x_0$), вектор (собственно, градиент) или матрица (матрица частных производных, вместо скалярного умножения - trace).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## полезные свойства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная константы:** $f(x) = a \\implies \\forall x_0 \\nabla f_{x_0} = 0$ - если функция константная, то её градиент равен 0 в любой точке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная линейного отображения:** если $f(x)$ - линейное отображение, то $f(x_0 + h) - f(x_0) = f(x_0) + f(h) - f(x_0) = f(h)$ ($f(x_0 + h) = f(x_0) + f(h)$ благодаря свойствам линейного отображения), т.е. $[D_{x_0}f](h) = f(h)$, градиент линейного отображения - само это линейное отображение. А линейное отображение - это умножение на матрицу, как мы помним (или на вектор). В общем, это аналог свойства производной умножения на константу:  \n",
    "- $(a \\cdot x)' = a $ ; \n",
    "- $f(x) = \\langle \\vec a , \\vec x \\rangle \\implies \\nabla f = \\vec a$ ; \n",
    "- $f(X) = AX \\implies [D_{x_0}f](H) = AH$ - здесь $A, X, H$ - матрицы. И \"градиент\", получается - матрица $A$ (но тут понятие градиента неприменимо, т.к. функция не скалярная, она в итоге вектор дает); \n",
    "- $f(X) = XA \\implies [D_{x_0}f](H) = HA$ - здесь также $A, X, H$ - матрицы. И \"градиент\" также матрица $A$, только умножение с другой стороны (но тут также понятие градиента неприменимо, т.к. функция не скалярная, она в итоге вектор дает).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейность производной:** если $f(x) = \\lambda u(x) + \\mu v(x)$, где $\\lambda , \\mu$ - скаляры, $u(x), v(x)$ - некоторые отображения, тогда:  \n",
    "$$[D_{x_0}f] = \\lambda [D_{x_0}u] + \\mu [D_{x_0}v]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная произведения:** если $f(x) = u(x) \\cdot v(x)$, где $u(x), v(x)$ - некоторые отображения, тогда:  $$[D_{x_0}f] = [D_{x_0}u] \\cdot v(x_0) + u(x_0) \\cdot [D_{x_0}v]$$  \n",
    "\n",
    "Это же правило сработает и для скалярного произведения:  \n",
    "$$[D_{x_0} \\langle u, v \\rangle] = \\langle [D_{x_0}u], v \\rangle + \\langle u, [D_{x_0}v] \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производная сложной функции:** если $f(x) = u(v(x))$, тогда:  \n",
    "$$[D_{x_0} u \\circ v](h) = [D_{v(x_0)} u]([D_{x_0} v](h))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дифференцирование перестановочно с линейным отображением:** если $f(x) = L(v(x))$, где $L$ - линейное отображение, тогда:  \n",
    "$$[D_{x_0} L \\circ v](h) = L([D_{x_0} v](h))$$  \n",
    "\n",
    "(это следуент из формулы производной сложной функции и особенностей производной линейного отображения, что оно само и является своим градиентом)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Свойства скалярного умножения:**  \n",
    "* $\\langle x, y \\rangle = \\langle y, x \\rangle$ - можно менять множители местами;  \n",
    "* $\\langle x, Ay \\rangle = \\langle A^Tx, y \\rangle$ - если есть домножение на матрицу - она переносится в другую сторону с транспонированием;  \n",
    "* $a \\cdot \\langle x, y \\rangle = \\langle a \\cdot x, y \\rangle = \\langle x, a \\cdot y \\rangle$ - если результат скалярного умножения умножается на скаляр ($a$), то его можно внести внутрь скалярного умножения, к любому из векторов, но только к одному!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вот как у нас считается лосс (SE, это как MSE, только без нормализации на длину вектора):  \n",
    "$$L(X, y, w) = \\|Xw - y\\|^2$$  \n",
    "\n",
    "Тут имеется в виду $L_2$ - норма, она же длина ветора (ошибок). Её можно расписать как скалярное умножение:  \n",
    "$$\\|Xw - y\\|^2 = \\langle Xw - y, Xw - y \\rangle$$  \n",
    "\n",
    "А теперь продифференцируем по $w$:  \n",
    "1. $ [D_{w_0} \\langle Xw - y, Xw - y \\rangle](h) = \\langle [D_{w_0} Xw - y](h), Xw_0 - y \\rangle + \\langle Xw_0 - y, [D_{w_0} Xw - y](h) \\rangle$ - по правилу производной произведения;  \n",
    "2. $\\langle [D_{w_0} Xw - y](h), Xw_0 - y \\rangle + \\langle Xw_0 - y, [D_{w_0} Xw - y](h) \\rangle = 2\\langle Xw_0 - y, [D_{w_0} Xw - y](h) \\rangle$ - т.к. мы можем менять местами множители в скалярном умножении. Ну и $a+a=2a$;  \n",
    "3. $[D_{w_0} Xw - y](h) = [D_{w_0} Xw](h) - [D_{w_0}y](h) = Xh - 0 = Xh$. Тут мы воспользовались линейностью производной (сделали из производной разности разность производных), а также тем, что производная константы - 0 ($y$ - это, по сути, константная функция), а производная линейного отображения - это само это линейное отображение (матрица $X$ как бы выступила градиентом, вынесли множитель);  \n",
    "4. $2\\langle Xw_0 - y, Xh \\rangle = 2\\langle X^T(Xw_0 - y), h \\rangle$ - перенесли множитель в другую часть скалярного произведения. Потому что нам нужно получить выражение вида $\\langle ?, h \\rangle$. Потому что тогда $?$ будет градиентом;  \n",
    "5. $2\\langle X^T(Xw_0 - y), h \\rangle = \\langle 2X^T(Xw_0 - y), h \\rangle$ - внесли множитель в скалярное умножение.  \n",
    "\n",
    "Всё, получили **градиент квадратичной ошибки**:  \n",
    "$$\\nabla_{w_0} L(X, y, w) = 2 X^T(Xw_0 - y)$$  \n",
    "\n",
    "Множитель $\\frac{1}{n}$, который делает MSE из SE, можно добавить тут: это всё равно константа, и она выносится за знак производной. Получается **градиент MSE**:  \n",
    "$$\\nabla_{w_0} MSE(X, y, w) = \\frac{2}{n} X^T(Xw_0 - y)$$  \n",
    "\n",
    "где $n$ - кол-во строк в обучающем датасете (длина $X$ и $y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь, чтобы решить линейную регрессию аналитически, надо этот градиент приравнять к нулю, и найти $w$:  \n",
    "$$2 X^T(Xw - y) = 0, w - ?$$  \n",
    "\n",
    "Решаем уравнение:  \n",
    "$2 X^T(Xw - y) = 0$ - исходное уравнение  \n",
    "$2 X^T Xw - 2 X^T y = 0$ - расврыли скобки  \n",
    "$2 X^T Xw = 2 X^T y $ - перенесли слагаемые в разные стороны  \n",
    "$X^T Xw = X^T y $ - разделили всё на 2 (если у нас будет $\\frac{2}{n}$, то оно тут же уйдет)  \n",
    "$w = (X^T X)^{-1}X^T y $ - перенесли множитель перед $w$  \n",
    "\n",
    "Итого, **аналитическое решение линейной регрессии**:  \n",
    "$$w_{*} = (X^T X)^{-1}X^T y$$  \n",
    "\n",
    "Здесь $w_{*}$ - оптимальные веса для модели.  \n",
    "\n",
    "Осталось, правда, доказать, что это действительно точка минимума. Мы могли бы найти вторую производную, но мы ограничимся замечанием о том, что _квадрат нормы является выпуклой функцией_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T09:17:23.229881Z",
     "start_time": "2022-05-31T09:17:23.226525Z"
    }
   },
   "source": [
    "### проблемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минус в том, что считать обратную матрицу - очень дорогая операция по вычислительной сложности. Нам получается $O(np^2)$, где $n$ - кол-во строк, $p$ - кол-во столбцов.  \n",
    "\n",
    "Одна операция вычисления градиента (один шаг градиентного спуска) весит примерно $O(np)$. Т.е. Одна итерация градиентного спуска в $p$ раз быстрее, чем аналитическое решение. Вот только итераций может понадобиться больше $p$. Но на помощь нам приходит стохастический градиентный спуск: мы на каждой итерации считаем градиент уже не $n$ строкам, а по какому-то небольшому батчу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Мультиколлинеарность](https://ru.wikipedia.org/wiki/%D0%9C%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BA%D0%BE%D0%BB%D0%BB%D0%B8%D0%BD%D0%B5%D0%B0%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C): если признаки сильно скоррелированы, то это приводит к неустойчивости оценок.  \n",
    "\n",
    "Неустойчивость выражается в увеличении статистической неопределенности — дисперсии оценок. Это означает, что конкретные результаты оценки могут сильно различаться для разных выборок несмотря на то, что выборки однородны.  \n",
    "\n",
    "Для обнаружения мультиколлинеарности факторов можно проанализировать непосредственно корреляционную матрицу факторов. Уже наличие больших по модулю (выше 0,7-0,8) значений коэффициентов парной корреляции свидетельствует о возможных проблемах с качеством получаемых оценок.  \n",
    "\n",
    "Однако, анализ парных коэффициентов корреляции недостаточен. Необходимо проанализировать коэффициенты детерминации регрессий факторов на остальные факторы ($R_{i}^{2}$). Рекомендуется рассчитывать показатель $VIF=1/(1-R_{j}^{2})$. Слишком высокие значения последнего означают наличие мультиколлинеарности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложнее о том же:  \n",
    "> Матрица $X^TX$, хотя почти всегда обратима в разумных задачах машинного обучения, но зачастую плохо обусловлена (например, если между признаками есть не точная, но приближённая линейная зависимость). Погрешность нахождения $w$ будет зависеть от квадрата [числа обусловленности](https://ru.wikipedia.org/wiki/%D0%A7%D0%B8%D1%81%D0%BB%D0%BE_%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8) матрицы $X$, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения $y$ могут приводить к катастрофическим изменениям $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sandbox]",
   "language": "python",
   "name": "conda-env-sandbox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
