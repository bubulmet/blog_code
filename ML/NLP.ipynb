{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем извлечь признаки из текста, упростим его.  \n",
    "\n",
    "Рассмотрим этапы предобработки текста:  \n",
    "* Токенизация (англ. tokenization) — разбиение текста на токены: отдельные фразы, слова, символы.  \n",
    "* Лемматизация (англ. lemmatization) — приведение слова к начальной форме (лемме).  \n",
    "\n",
    "Функция лемматизации русского текста есть в библиотеках:  \n",
    "- pymorphy2 (англ. python morphology, «морфология для Python»),  \n",
    "- UDPipe (англ. universal dependencies pipeline, «конвейер для построения общих зависимостей»),  \n",
    "- pymystem3.  \n",
    "\n",
    "https://habr.com/ru/post/503420/  \n",
    "\n",
    "Чтобы машины воспринимали слова, картинки или аудио, их преобразовывают в векторный вид. Когда работают с текстом, его тоже переводят в векторный формат, или векторные представления. Частный случай этих представлений — **word embeddings** (англ. «слова-вложения»; «эмбеддинги»). Работают они так: сложная структура (текст) вкладывается в более простую — вектор.  \n",
    "\n",
    "Векторы-эмбеддинги содержат данные о соотношении разных слов и их свойствах. Привычное понимание свойства слова, его смысла и контекста справедливо и для машинного обучения.  \n",
    "\n",
    "Свойства — это скрытые смыслы слова. Допустим, «моряк» содержит смыслы: «мужчина», «профессия», «человек» и «морской». Смысл, или семантика слова, — это лексическое значение слова, его отличие от других слов. Но слово обычно не живёт само по себе, его окружают другие слова. Например, «лента» во фразе: «Красная лента в каштановых волосах» отличается от «ленты» из предложения: «Положите продукты на ленту перед кассой». Именно контекст и определяет смысл слов.  \n",
    "\n",
    "Эти понятия пригодятся нам в определении близости слов в их векторном представлении. Так близкие, или похожие векторы, могут отображать похожие слова. Сходство векторов рассчитывается знакомым вам евклидовым расстоянием: чем меньше расстояние, тем сильнее похожи векторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conda activate sandbox`  \n",
    "\n",
    "https://pypi.org/project/pymystem3/  \n",
    "  \n",
    "`pip install pymystem3`  \n",
    "\n",
    "Один раз после установки _nltk_ надо скачать стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:29:18.072270Z",
     "start_time": "2022-04-01T09:29:18.066368Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(у меня в _sandbox_ это уже было сделано)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **RuBERT**'a:  \n",
    "\n",
    "```\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "\n",
    "https://huggingface.co/docs/transformers/index   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:09:56.010312Z",
     "start_time": "2022-04-02T05:09:55.378265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T13:59:56.766938Z",
     "start_time": "2022-04-01T13:59:56.760262Z"
    }
   },
   "outputs": [],
   "source": [
    "PREFIX = 'https://code.s3.yandex.net' # PREFIX + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "русскоязычный корпус коротких текстов **RuTweetCorp**: http://study.mokoron.com/  \n",
    "\n",
    "Перед вами уменьшенный датасет — 5000 записей. Каждая запись содержит текст твита и оценку его тональности. Если пост позитивный, то метка «1», если негативный — «0». (Твиты могут содержать обсценную лексику)  \n",
    "\n",
    "Выгрузить данные моджно в задании про sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:00.732158Z",
     "start_time": "2022-04-02T05:15:00.670556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>positive</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "      <td>хоть я и школотый но поверь у мы то же самый о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "      <td>да весь таки он немного похожий на он но мой м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ну ты идиотка я испугаться за ты</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1</td>\n",
       "      <td>кто то в угол сидеть и погибать от голод а мы ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\r\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>вот что значит страшилка но блин посмотреть ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  positive  \\\n",
       "0  @first_timee хоть я и школота, но поверь, у на...         1   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...         1   \n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...         1   \n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...         1   \n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\r\\...         1   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  хоть я и школотый но поверь у мы то же самый о...  \n",
       "1  да весь таки он немного похожий на он но мой м...  \n",
       "2                  ну ты идиотка я испугаться за ты   \n",
       "3  кто то в угол сидеть и погибать от голод а мы ...  \n",
       "4  вот что значит страшилка но блин посмотреть ве...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./files/tweets_lemm_train.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:01.626269Z",
     "start_time": "2022-04-02T05:15:01.579813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @tiredfennel: если криса так интересуют дет...</td>\n",
       "      <td>если крис так интересовать ребёнок то либо они...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@xsealord по 200 руб. в месяц можно разместить...</td>\n",
       "      <td>по рубль в месяц можно разместить ссылка на те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@haosANDlaw @Etishkindyx учитывая, что сейчас ...</td>\n",
       "      <td>учитывать что сейчас преобладать один половина...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Товарищ    :) Но я никак не могу отдельно не о...</td>\n",
       "      <td>товарищ но я никак не мочь отдельно не отметит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @BodyaNick: Квн был отличный !) Оооочень по...</td>\n",
       "      <td>квн быть отличный оооочень понравиться женский...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @tiredfennel: если криса так интересуют дет...   \n",
       "1  @xsealord по 200 руб. в месяц можно разместить...   \n",
       "2  @haosANDlaw @Etishkindyx учитывая, что сейчас ...   \n",
       "3  Товарищ    :) Но я никак не могу отдельно не о...   \n",
       "4  RT @BodyaNick: Квн был отличный !) Оооочень по...   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  если крис так интересовать ребёнок то либо они...  \n",
       "1  по рубль в месяц можно разместить ссылка на те...  \n",
       "2  учитывать что сейчас преобладать один половина...  \n",
       "3  товарищ но я никак не мочь отдельно не отметит...  \n",
       "4  квн быть отличный оооочень понравиться женский...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./files/tweets_lemm_test.csv')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T08:36:08.888480Z",
     "start_time": "2022-04-01T08:36:08.870464Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = train['text'].values.astype('U') # Переведём тексты в Unicode\n",
    "\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T08:43:12.730325Z",
     "start_time": "2022-04-01T08:43:12.720062Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    m = Mystem()\n",
    "    lemm_list = m.lemmatize(text)\n",
    "    lemm_text = \"\".join(lemm_list)\n",
    "\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T08:43:13.662841Z",
     "start_time": "2022-04-01T08:43:13.076168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@first_timee хоть я и школоть, но поверять, у мы то же самый :D общество профилирующий предмет тип)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T08:43:14.685620Z",
     "start_time": "2022-04-01T08:43:14.672001Z"
    }
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    clean_text = re.sub(r'[^а-яА-ЯёЁ]',' ',text)\n",
    "    clean_text = \" \".join(clean_text.split())\n",
    "\n",
    "    return(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T08:43:15.612888Z",
     "start_time": "2022-04-01T08:43:15.034325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хоть я и школоть но поверять у мы то же самый общество профилировать предмет тип\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(clear_text(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words, stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:00:44.092724Z",
     "start_time": "2022-04-01T09:00:43.924023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка без учёта стоп-слов: (5000, 18697)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"Размер мешка без учёта стоп-слов:\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:00:50.586564Z",
     "start_time": "2022-04-01T09:00:50.575376Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:01:10.546201Z",
     "start_time": "2022-04-01T09:01:10.426502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка с учётом стоп-слов: (5000, 18555)\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk_stopwords.words('russian')\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stop_words)\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"Размер мешка с учётом стоп-слов:\", bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-граммы (по N **слов**, не символов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:06:37.071298Z",
     "start_time": "2022-04-01T09:06:36.844408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (5000, 41914)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "n_gramm = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"Размер:\", n_gramm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка важности слова определяется величиной $TF-IDF$ (от англ. _term frequency_, «частота терма, или слова»; _inverse document frequency_, «обратная частота документа, или текста»).  \n",
    "\n",
    "То есть $TF$ отвечает за количество упоминаний слова в отдельном тексте, а $IDF$ отражает частоту его употребления во всём корпусе.  \n",
    "\n",
    "$$TF-IDF = TF * IDF$$  \n",
    "\n",
    "$TF = \\frac{t}{n}$, где $t$ (от англ. _term_) — количество употребления слова; $n$ — общее число слов в тексте.  \n",
    "$IDF = \\log{\\frac{D}{d}}$, где $D$ - общее число текстов в корпусе; d - количество текстов, в которых данное слово встречается.\n",
    "\n",
    "$IDF$ нужна в формуле, чтобы уменьшить вес слов, наиболее распространённых в любом другом тексте заданного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:14:20.838712Z",
     "start_time": "2022-04-02T05:14:20.477649Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk_stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T09:17:05.609556Z",
     "start_time": "2022-04-01T09:17:05.435533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (5000, 18555)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "print(\"Размер матрицы:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Передав `TfidfVectorizer()` аргумент `ngram_range`, можно рассчитать N-граммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ тональности текста, или сентимент-анализ (от англ. sentiment, «настроение»), выявляет эмоционально окрашенные слова. Этот инструмент помогает компаниям оценивать, например, реакцию на запуск нового продукта в интернете. На разбор тысячи отзывов человек потратит несколько часов, а компьютер — пару минут.  \n",
    "\n",
    "Оценить тональность — значит отметить текст как позитивный или негативный. То есть мы решаем задачу классификации, где целевой признак равен «1» для положительного текста и «0» для отрицательного. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:09.347868Z",
     "start_time": "2022-04-02T05:15:09.341389Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:11.455195Z",
     "start_time": "2022-04-02T05:15:11.356522Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nltk_stopwords.words('russian')\n",
    "\n",
    "tf_idf_transformer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf_transformer = tf_idf_transformer.fit(train['lemm_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:20.125826Z",
     "start_time": "2022-04-02T05:15:20.028097Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tf_idf_transformer.transform(train['lemm_text'])\n",
    "X_test = tf_idf_transformer.transform(test['lemm_text'])\n",
    "\n",
    "y_train = train['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:21.679981Z",
     "start_time": "2022-04-02T05:15:21.583189Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:26.460702Z",
     "start_time": "2022-04-02T05:15:26.412814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @tiredfennel: если криса так интересуют дет...</td>\n",
       "      <td>если крис так интересовать ребёнок то либо они...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@xsealord по 200 руб. в месяц можно разместить...</td>\n",
       "      <td>по рубль в месяц можно разместить ссылка на те...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@haosANDlaw @Etishkindyx учитывая, что сейчас ...</td>\n",
       "      <td>учитывать что сейчас преобладать один половина...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Товарищ    :) Но я никак не могу отдельно не о...</td>\n",
       "      <td>товарищ но я никак не мочь отдельно не отметит...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @BodyaNick: Квн был отличный !) Оооочень по...</td>\n",
       "      <td>квн быть отличный оооочень понравиться женский...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @tiredfennel: если криса так интересуют дет...   \n",
       "1  @xsealord по 200 руб. в месяц можно разместить...   \n",
       "2  @haosANDlaw @Etishkindyx учитывая, что сейчас ...   \n",
       "3  Товарищ    :) Но я никак не могу отдельно не о...   \n",
       "4  RT @BodyaNick: Квн был отличный !) Оооочень по...   \n",
       "\n",
       "                                           lemm_text  positive  \n",
       "0  если крис так интересовать ребёнок то либо они...         1  \n",
       "1  по рубль в месяц можно разместить ссылка на те...         0  \n",
       "2  учитывать что сейчас преобладать один половина...         0  \n",
       "3  товарищ но я никак не мочь отдельно не отметит...         0  \n",
       "4  квн быть отличный оооочень понравиться женский...         1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['positive'] = model.predict(X_test)\n",
    " \n",
    "test.to_csv('./files/predictions', index=False)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:47.009877Z",
     "start_time": "2022-04-02T05:15:46.946854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6231414352420743"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "X_train_train, X_train_valid, y_train_train, y_train_valid = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    test_size=0.25, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train_train, y_train_train)\n",
    "\n",
    "y_pred = model.predict(X_train_valid)\n",
    "\n",
    "metrics.roc_auc_score(y_train_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим популярный метод построения языковых представлений — word2vec (от англ. word to vector, «слова к вектору»).  \n",
    "\n",
    "Упрощённо разберём, как работает word2vec. Из примера с амадиной и тупиком вы узнали, что сходство этих слов определяется соседством с «красным клювом». А «муравьед» и «ленивец» часто встречаются в предложениях о Перу. То есть смысл слов определяется их контекстом.  \n",
    "\n",
    "Тогда задача word2vec — предсказать: соседи или нет — заданные слова. Слова считаются соседями, если находятся в одном «окне» (максимальном расстоянии между словами). Выходит, пара слов — это признаки, а являются ли они соседями — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/post/446530/ - Word2vec в картинках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT** (от англ. _Bidirectional Encoder Representations from Transformers_, «двунаправленная нейронная сеть-кодировщик») — нейронная сеть для создания модели языка. Её разработали в компании Google, чтобы повысить релевантность результатов поиска. Этот алгоритм понимает контекст запросов, а не просто анализирует фразы. Для машинного обучения она ценна тем, что помогает строить векторные представления. Причём в анализе текстов применяют уже предобученную на большом корпусе модель. Такие предобученные версии **BERT** годятся для работы с текстами на 104 языках мира, включая русский.  \n",
    "\n",
    "**BERT** — это результат эволюции модели **word2vec**. В ходе её развития были придуманы и другие модели: **FastText** (англ. «быстрый текст»), **GloVe** (англ. _Global Vectors for Word Representation_, «глобальные векторы для языкового представления»), **ELMO** (англ. _Embeddings from Language Models_, «вложения языковых моделей») и **GPT** (англ. _Generative Pre-Training Transformer_, «предобученный трансформер для генерации»). Сейчас самые точные — это **BERT** и **GPT-3**, которого нет в открытом доступе.  \n",
    "\n",
    "**BERT** учитывает контекст не только соседних слов, но и более дальних родственников. Работает так:\n",
    "На входе модель получает, например, такую фразу: `«Красный клюв тупика [MASK] на голубом [MASK]»`, где MASK (англ. «маска») — это неизвестные слова, будто закрытые маской. Модель должна угадать эти спрятанные слова.\n",
    "Модель обучается определять, связаны ли в предложении слова между собой. У нас были скрыты такие слова: «мелькнул» и «небе». Модель должна понять, что одно слово — продолжение другого. Скажем, если вместо «мелькнул» спрятать слово «прополз», то связи модель не найдёт.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед вами большой датасет с твитами. Нужно научиться определять, какие твиты негативной тональности, а какие — позитивной. Чтобы решить эту задачу, из открытого репозитория [DeepPavlov](http://docs.deeppavlov.ai/en/master/features/models/bert.html) возьмём модель **RuBERT**, обученную на разговорном русскоязычном корпусе.  \n",
    "\n",
    "Решим эту задачу на **PyTorch** (англ. «факел для Python»). Глубоко разбираться в средствах этой библиотеки мы не будем. Она применяется в задачах обработки естественного текста и компьютерного зрения. А нам нужна для работы с моделями типа BERT. Они находятся в библиотеке _transformers_ (англ. «трансформеры»): https://huggingface.co/transformers/.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем всякие заготовки. Пошел [сюда](http://docs.deeppavlov.ai/en/master/features/models/bert.html), и скачал две версии (в формате `deeppavlov_pytorch`): \n",
    "* **RuBERT** was trained on the Russian part of Wikipedia and news data. We used this training data to build vocabulary of Russian subtokens and took multilingual version of BERT-base as initialization for RuBERT 1.  \n",
    "* **Conversational RuBERT** was trained on OpenSubtitles 5, Dirty, Pikabu, and Social Media segment of Taiga corpus 8. We assembled new vocabulary for Conversational RuBERT model on this data and initialized model with RuBERT.  \n",
    "\n",
    "Сохранил в `./files/rubert/` и `./files/ru_conversational/` соответственно. Там всё:  \n",
    "* `vocab.txt`\n",
    "* `bert_config.json`\n",
    "* `pytorch_model.bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Погнали!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:55.463919Z",
     "start_time": "2022-04-02T05:15:55.455946Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:15:59.640280Z",
     "start_time": "2022-04-02T05:15:58.125636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./files/ru_conversational/pytorch_model.bin were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "path_prefix = './files/ru_conversational/' # './files/rubert/' './files/ru_conversational/'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer(vocab_file=path_prefix+'vocab.txt')\n",
    "config = transformers.BertConfig.from_json_file(path_prefix+'bert_config.json')\n",
    "model = transformers.BertModel.from_pretrained(path_prefix+'pytorch_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:16:06.148133Z",
     "start_time": "2022-04-02T05:16:06.142075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1094, 4980, 3373, 1034, 6037, 323, 73634, 10316, 102]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректной работы модели мы указали аргумент `add_special_tokens` (англ. «добавить специальные токены»), равный `True`. Это значит, что к любому преобразуемому тексту добавляется токен начала (101) и токен конца текста (102).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:16:09.385988Z",
     "start_time": "2022-04-02T05:16:09.359149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@bitchpleasemary верхнее эт ад((((((((\\r\\nя ры...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>кстати. я вчера узнала, что настоящий 3д принт...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>посмотрела  сопливую мелодраму и влюбилась в г...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Anuta1961 ахахаха Огооонь:))) как там твоя ма...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_13417 ахаха. Хорошо. А я ток домой. С 8свали...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  positive\n",
       "0  @bitchpleasemary верхнее эт ад((((((((\\r\\nя ры...         0\n",
       "1  кстати. я вчера узнала, что настоящий 3д принт...         1\n",
       "2  посмотрела  сопливую мелодраму и влюбилась в г...         0\n",
       "3  @Anuta1961 ахахаха Огооонь:))) как там твоя ма...         1\n",
       "4  @_13417 ахаха. Хорошо. А я ток домой. С 8свали...         1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтобы долго не обрабатывать - возьмем маленький сэмпл\n",
    "df_tweets = train[['text', 'positive']].sample(400).reset_index(drop=True)\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:17:02.145502Z",
     "start_time": "2022-04-02T05:17:01.971124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [101, 168, 18196, 2537, 22750, 17408, 15190, 2...\n",
       "1    [101, 2451, 132, 358, 4806, 19914, 128, 825, 2...\n",
       "2    [101, 29968, 25733, 2111, 57255, 878, 322, 580...\n",
       "3    [101, 168, 5335, 2598, 76511, 20924, 139, 6062...\n",
       "4    [101, 168, 230, 19058, 4545, 34843, 132, 1643,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = df_tweets['text'].apply( # BERT'у не нужна лемматизация!\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True)\n",
    ")\n",
    "\n",
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:17:11.645975Z",
     "start_time": "2022-04-02T05:17:11.631003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# максимальная возможная длина твита в токенах\n",
    "max_len = tokenized.apply(lambda x: len(x)).max()\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:17:15.366551Z",
     "start_time": "2022-04-02T05:17:15.345435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 86)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заполняем нулями до максимальной длины, чтоб одна форма была\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:17:32.616000Z",
     "start_time": "2022-04-02T05:17:32.601434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 86)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ставим attention=1 только там, где реальный токен, а не заполняющий дырки ноль\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь к собственно эмбеддингам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:18:39.422752Z",
     "start_time": "2022-04-02T05:18:01.905712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879c98901cf349008e1194f26662fd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 05:18:11.197495\n",
      "1 05:18:20.525155\n",
      "2 05:18:30.582974\n",
      "3 05:18:39.419351\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 # по батчам будем обрабатывать\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "    # преобразуем данные в torch Tensor\n",
    "    batch = torch.LongTensor(\n",
    "        padded[batch_size*i:batch_size*(i+1)]\n",
    "    )\n",
    "    # преобразуем маску в torch Tensor\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "    \n",
    "    with torch.no_grad(): # так быстрее\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy()) # см. пояснения ниже\n",
    "    \n",
    "    # verbose\n",
    "    print(i, pd.to_datetime('now').time().isoformat())\n",
    "    \n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_embeddings[0][:,0,:]`: почему так?  \n",
    "\n",
    "в `batch_embeddings` - два тензора. Первый - это *last_hidden_state*, значения эмбеддингов для каждого токена каждого твита из батча. Второй - это *pooler_output*, специально обработанный выход для первого токена (того самого, который и мы берем в итоге): + полносвязный линейный слой и тангенс-активация. Полное описание того, что возвращает модель, есть [тут](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward.returns). В общем, берем первый, *last_hidden_state*.\n",
    "\n",
    "`[:,0,:]` - это срез (slice) трёхмерного массива. Эмбеддинг превращает каждое слово твита (токен) в вектор длины 768. То есть в нашем батче из 100 твитов, каждый ширины 133 (т.к. мы заполнили все, что было короче 133 токенов, нулями, в padded) появляется третье измерение: размерность эмбеддинга, \"глубина\". То есть по сути, для каждого твита мы берем эмбеддинг только первого слова (все 100 твитов батча, все 768 значений вектора эмбеддинга, но только 0-е слово).  \n",
    "\n",
    "На самом деле это не первое слово твита, т.к. при вызове `tokenizer.encode(x, add_special_tokens=True)` мы добавляем перед токенами самого твита специальный токен `[CLF]`. Это специальный токен, собирающий всю необходимую информацию для классификации. Его эмбеддинг мы и берем, и его же эмбеддинг (немного доработанный) возвращается в *pooler_output*, втором тензоре.  \n",
    "\n",
    "В общем, запомнить нужно следующее:\n",
    "* первый токен в токенизированном твите (после `tokenizer.encode()` если `add_special_tokens=True`) - это специальный токен `[CLF]`, чей эмбеддинг по-сути является эмбеддингом целого предложения для задачи классификации.\n",
    "* модель возвращает эмбеддинги для каждого токена (*last_hidden_state*), а также специально дообработанный эмбеддинг специального токена `[CLF]` (*pooler_output*). Т.е. два тензора, первый трёхмерный, второй - двумерный (*last_hidden_state* и *pooler_output*).\n",
    "* для дальнейшей классификации в качестве эмбеддинга всего твита можно использовать `last_hidden_state[:,0,:]` или `pooler_output`. Там примерно одно и то же, только в `pooler_output` эмбеддинг  `[CLF]` (`last_hidden_state[:,0,:]`) немного дообработан (+ полносвязный линейный слой и тангенс-активация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T05:19:05.276067Z",
     "start_time": "2022-04-02T05:19:05.139183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9189244663382594"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    df_tweets['positive'], \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stepik.org/course/54098/syllabus  \n",
    "https://github.com/Samsung-IT-Academy/stepik-dl-nlp  \n",
    "https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task2_word_embeddings.ipynb  \n",
    "https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task9_bert_sentiment_analysis.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rusvectores.org/ru/models/  \n",
    "geowac_tokens_none_fasttextskipgram_300_5_2020  \n",
    "https://fasttext.cc/  \n",
    "https://github.com/facebookresearch/fastText  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T10:28:07.317440Z",
     "start_time": "2022-04-02T10:28:02.862679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18202202e-04,  2.35366239e-03,  1.78556633e-03,\n",
       "        -1.04790344e-03, -1.13029417e-03],\n",
       "       [ 3.35591048e-01, -1.10159373e+00, -1.14366546e-01,\n",
       "        -6.55330002e-01, -1.21734643e+00],\n",
       "       [-5.56692481e-04, -2.73203687e-03,  8.11694845e-05,\n",
       "        -4.20838507e-04, -2.95227626e-03],\n",
       "       [-2.45295785e-04,  1.30942592e-03, -1.32767402e-03,\n",
       "         3.04132444e-03, -2.79921456e-03],\n",
       "       [-1.08011405e-03,  3.30130872e-03, -2.84952554e-03,\n",
       "         1.47060910e-03,  7.80855771e-04]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = np.load(\n",
    "    file='./files/214/model.model.vectors_ngrams.npy', \n",
    "    mmap_mode=None, \n",
    "    allow_pickle=False, \n",
    "    fix_imports=True, \n",
    "    encoding='ASCII'\n",
    ")\n",
    "\n",
    "lol[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T10:28:09.969296Z",
     "start_time": "2022-04-02T10:28:09.952571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0940178e-03,  3.2052917e-03,  3.1521744e-03,  2.2887776e-03,\n",
       "        -2.8068325e-03],\n",
       "       [ 4.0677685e-01, -2.2524649e-01,  4.4718465e-01, -1.7501085e-01,\n",
       "         4.2675009e-01],\n",
       "       [-2.5399609e-03, -3.2787060e-03,  3.1196531e-03,  3.5778832e-04,\n",
       "         2.5944707e-03],\n",
       "       [-7.4223144e-04,  6.9249352e-04,  1.5482459e-03, -1.0184679e-03,\n",
       "        -6.1467348e-04],\n",
       "       [ 2.3629216e-03, -1.0944166e-03, -1.4347958e-03,  2.7006823e-03,\n",
       "         2.0544571e-03]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[:5, -5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# natasha navec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/natasha/navec  \n",
    "\n",
    "Скачал `navec_hudlit_v1_12B_500K_300d_100q.tar`, положил в `./files`. Попробуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:22:58.223575Z",
     "start_time": "2022-04-02T11:22:54.398453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting navec\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy in /home/bubulmet/miniconda3/envs/sandbox/lib/python3.8/site-packages (from navec) (1.22.3)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] Нет такого файла или каталога: '/home/bubulmet/miniconda3/envs/sandbox/lib/python3.8/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: navec\n",
      "Successfully installed navec-0.10.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:27:33.566077Z",
     "start_time": "2022-04-02T11:27:33.513481Z"
    }
   },
   "outputs": [],
   "source": [
    "from navec import Navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:27:34.825884Z",
     "start_time": "2022-04-02T11:27:34.111166Z"
    }
   },
   "outputs": [],
   "source": [
    "navec = Navec.load(\n",
    "    './files/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:27:36.859180Z",
     "start_time": "2022-04-02T11:27:36.818282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3955571 ,  0.11600914,  0.24605067, -0.35206917, -0.08932345,\n",
       "        0.3382279 , -0.5457616 ,  0.07472657, -0.4753835 , -0.3330848 ,\n",
       "        0.1449912 , -0.13690177,  0.0840969 , -0.2141802 , -0.57312167,\n",
       "       -0.06113251,  0.04558022, -0.30147526,  0.55966735, -0.30724582,\n",
       "       -0.23867525, -0.28796813, -0.10781458,  0.09957517,  0.25810853,\n",
       "        0.05649512, -0.03052731,  0.06966446,  0.01507293, -0.14274776,\n",
       "       -0.40885502,  0.2893703 ,  0.3201632 , -0.27072856, -0.04702847,\n",
       "        0.41056094,  0.51552117, -0.27452162, -0.10154261,  0.41732824,\n",
       "        0.15827584, -0.03746929,  0.17745009, -0.03559239, -0.29721352,\n",
       "       -0.39473224, -0.04143994,  0.05495927,  0.47845373, -0.32670122,\n",
       "        0.14475909,  0.35896015, -0.3800541 ,  0.29924598, -0.31741822,\n",
       "       -0.71888554, -0.35691768, -0.2958643 , -0.37184098,  0.08903536,\n",
       "       -0.12709911, -0.06632375,  0.03680592,  0.27049237,  0.00893382,\n",
       "       -0.20365159, -0.27380955,  0.08020782,  0.12610178,  0.04507966,\n",
       "       -0.56111956, -0.62939566, -0.32951435, -0.1810639 , -0.28960884,\n",
       "        0.6197792 , -0.10272317, -0.12893517,  0.2511102 , -0.04666989,\n",
       "       -0.01017368,  0.2778579 , -0.06260373,  0.32465425,  0.61492896,\n",
       "        0.13165517,  0.2256508 ,  0.4113643 , -0.07210559,  0.21978192,\n",
       "        0.2030308 , -0.4252658 , -0.03872509, -0.3849392 , -0.17426725,\n",
       "        0.65441537, -0.54959536, -0.41553697,  0.03038502,  0.63821596,\n",
       "        0.02551245, -0.10792938,  0.4714141 ,  0.17638063,  0.21021639,\n",
       "       -0.22764762, -0.07045569, -0.27518165,  0.1774276 ,  0.23115063,\n",
       "       -0.59743494,  0.44871888, -0.10782384,  0.37884295,  0.29360834,\n",
       "        0.0363479 ,  0.22075066, -0.34550273, -0.2987647 , -0.14842781,\n",
       "       -0.23698306, -0.14348198, -0.0511187 , -0.14244387,  0.20397018,\n",
       "        0.25838128,  0.4832477 , -0.22960119,  0.01627262, -0.21261746,\n",
       "       -0.2715558 ,  0.28467602,  0.15231182, -0.09285944,  0.28947443,\n",
       "       -0.06843787,  0.1255881 ,  0.8702048 ,  0.02227183,  0.27699545,\n",
       "        0.19527818,  0.38627282, -0.01307742, -0.12605608,  0.06644178,\n",
       "        0.02891991, -0.20725605,  0.17995903, -0.1606334 ,  0.27738062,\n",
       "       -0.44023108,  0.13876231, -0.17100142,  0.07651781, -0.31987593,\n",
       "        0.39109808, -0.64132386, -0.18748127, -0.15217304, -0.33366784,\n",
       "       -0.23702534, -0.09647428,  0.14715166,  0.50744605, -0.47037542,\n",
       "        0.22670864,  0.01208101, -0.5175762 ,  0.28740397,  0.24730249,\n",
       "       -0.14597584,  0.71664655, -0.10583282, -0.10196099, -0.26773652,\n",
       "       -0.30576602,  0.31484523, -0.33663896, -0.4439698 ,  0.13530843,\n",
       "       -0.48986742, -0.04291547,  0.04542742, -0.12595604,  0.01892297,\n",
       "       -0.16023673, -0.09223235, -0.3519353 , -0.27544725, -0.35941103,\n",
       "        0.26816747, -0.26366055,  0.09045722, -0.09209683, -0.19774392,\n",
       "        0.4626923 ,  0.11606661,  0.13637708,  0.0600035 ,  0.3952663 ,\n",
       "       -0.11322273,  0.12690416,  0.4223611 , -0.3319263 , -0.0074226 ,\n",
       "        0.181907  , -0.5575815 , -0.24921872,  0.5261132 , -0.17298555,\n",
       "        0.4189706 ,  0.00610934, -0.8102452 ,  0.13547905, -0.07609115,\n",
       "       -0.3848198 , -0.15416847,  0.46580663, -0.00973997,  0.41822353,\n",
       "       -0.0420084 ,  0.22273165, -0.6414529 ,  0.18634477, -0.1320177 ,\n",
       "       -0.3993432 , -0.29942054, -0.17588404,  0.01930285,  0.15464629,\n",
       "        0.0271406 ,  0.32141694,  0.26384956,  0.32256746, -0.0999136 ,\n",
       "       -0.00914712,  0.04311815,  0.06883949, -0.26403832, -0.23895177,\n",
       "        0.06563807, -0.32052973,  0.51667506,  0.28173164, -0.03728829,\n",
       "        0.10617348,  0.46699637, -0.15934104, -0.00895077,  0.03029792,\n",
       "       -0.0790227 , -0.15837127,  0.07571064,  0.08949807,  0.31985927,\n",
       "        0.22785452, -0.04943721, -0.39527982, -0.47163686,  0.20657642,\n",
       "        0.01421705,  0.4552891 , -0.13419099, -0.03937298, -0.12779118,\n",
       "        0.30422038,  0.00630774, -0.61479425,  0.30588475, -0.1631542 ,\n",
       "        0.47571513,  0.23350583, -0.03703165,  0.6147397 , -0.6354517 ,\n",
       "        0.04949358, -0.23769939, -0.0374764 , -0.20668483,  0.03984997,\n",
       "        0.05531057, -0.14646474, -0.7818065 , -0.1705449 , -0.47148925,\n",
       "        0.7534342 ,  0.16350834, -0.4402775 ,  0.3062885 , -0.2754479 ,\n",
       "        0.7068775 ,  0.74453825,  0.44501626, -0.32818314, -0.6157321 ,\n",
       "        0.24681088,  0.20351061, -0.4685785 , -0.20134608, -0.12309294],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding\n",
    "navec['навек']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:28:15.216298Z",
     "start_time": "2022-04-02T11:28:15.202036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "'нааавееек' in navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:28:27.434322Z",
     "start_time": "2022-04-02T11:28:27.427763Z"
    }
   },
   "outputs": [],
   "source": [
    "navec.get('нааавееек') # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:29:04.747178Z",
     "start_time": "2022-04-02T11:29:04.736580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225823"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of word\n",
    "navec.vocab['навек']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:29:25.359107Z",
     "start_time": "2022-04-02T11:29:25.344517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with check\n",
    "navec.vocab.get('наааавеeeк', navec.vocab.unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:29:46.872181Z",
     "start_time": "2022-04-02T11:29:46.846964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.14312136e-01,  3.70287180e-01,  1.36796311e-01, -1.86535835e-01,\n",
       "       -4.91572991e-02, -1.88751370e-02, -1.32190725e-02,  3.57939489e-02,\n",
       "       -3.97918969e-02,  1.94128811e-01, -8.25866908e-02,  2.47690390e-04,\n",
       "        2.88263001e-02,  2.01173663e-01, -1.54272586e-01, -1.13935187e-01,\n",
       "       -4.74858470e-02, -7.92295299e-03,  6.50205165e-02, -3.78477909e-02,\n",
       "        6.24999329e-02,  2.49566153e-01,  1.03294946e-01, -2.11493388e-01,\n",
       "       -1.73085947e-02, -2.82477215e-02, -6.87575415e-02, -9.21097770e-02,\n",
       "        8.71437322e-03, -1.69095173e-01, -5.73454238e-02,  4.21022065e-02,\n",
       "       -5.24346411e-01, -1.58332035e-01,  5.83604947e-02, -6.78519439e-03,\n",
       "       -6.93208054e-02, -5.74708311e-03, -1.20353170e-01, -4.40001450e-02,\n",
       "        4.75032702e-02, -2.33378902e-01, -1.33015737e-01,  1.27385199e-01,\n",
       "       -7.16302916e-02,  1.28748834e-01,  1.13330811e-01,  1.26265138e-02,\n",
       "        5.89972734e-02,  2.43283421e-01, -8.16499963e-02,  2.72306442e-01,\n",
       "        1.67372063e-01, -1.33618921e-01,  1.89889565e-01, -9.01413932e-02,\n",
       "        1.14899948e-01, -5.96789792e-02,  6.26452938e-02, -1.41223237e-01,\n",
       "       -2.08740175e-01,  3.32493484e-01,  2.14649692e-01, -3.34672779e-02,\n",
       "        5.25368713e-02, -1.45456240e-01,  1.46174431e-01,  1.52270079e-01,\n",
       "       -9.52247530e-03,  2.50804238e-02,  2.03919202e-01, -4.77523766e-02,\n",
       "        6.55542389e-02, -4.80662771e-02, -2.27324516e-02,  1.13046184e-01,\n",
       "       -1.54620215e-01,  4.85879853e-02, -9.92703661e-02,  1.33642517e-02,\n",
       "        3.87403667e-01,  1.51388884e-01,  4.65942472e-02,  3.72258306e-01,\n",
       "        8.34996030e-02, -1.95314333e-01,  4.20905292e-01, -1.83610618e-02,\n",
       "        8.96160752e-02,  9.06159878e-02, -1.41039923e-01,  1.38926119e-01,\n",
       "        1.75671160e-01, -4.77052480e-02,  6.52849749e-02, -1.81734160e-01,\n",
       "        1.34497583e-01,  4.52925228e-02,  1.77893162e-01,  8.24629366e-02,\n",
       "       -2.26429254e-02, -7.81486835e-03,  1.04925431e-01,  1.10557005e-01,\n",
       "       -2.29530305e-01,  9.88782197e-02,  3.10462683e-01, -8.70207101e-02,\n",
       "       -1.03182040e-01,  2.13678405e-01, -2.50720121e-02,  1.29431337e-01,\n",
       "        1.54199198e-01, -2.19486684e-01,  3.71575169e-02, -7.83453137e-02,\n",
       "        7.56102279e-02, -1.48542151e-01, -1.30579412e-01, -1.72655612e-01,\n",
       "       -2.32030213e-01,  1.96986243e-01, -1.19844198e-01, -1.61239907e-01,\n",
       "       -1.23323025e-02,  1.08766764e-01,  1.77958250e-01,  1.16608568e-01,\n",
       "        5.23438416e-02,  8.04402903e-02,  1.20596744e-01,  2.01546252e-01,\n",
       "       -2.42847838e-02, -5.51940240e-02,  3.43268484e-01, -1.74818859e-01,\n",
       "       -1.89623818e-01,  1.73824951e-02, -5.79115637e-02,  7.34661222e-02,\n",
       "       -1.77343175e-01,  9.75643843e-02,  3.94618399e-02,  5.85069545e-02,\n",
       "        8.76323134e-02,  1.29697844e-01, -9.66188908e-02, -7.55561888e-02,\n",
       "        4.56631891e-02, -7.39202276e-02, -1.87734172e-01, -2.26111129e-01,\n",
       "       -8.17668959e-02, -1.41328156e-01, -4.58951257e-02, -6.45612031e-02,\n",
       "       -1.69128492e-01,  1.30283371e-01, -8.07456896e-02, -1.02156378e-01,\n",
       "        2.47294649e-01, -5.10989875e-02,  3.03961843e-01, -4.96661589e-02,\n",
       "        5.26514724e-02, -4.51278463e-02, -7.08152354e-02, -9.64756496e-03,\n",
       "        1.41175479e-01, -4.36366862e-03, -1.09772854e-01, -1.34531349e-01,\n",
       "       -5.43047376e-02, -2.91877270e-01, -4.83813137e-02, -1.34292915e-01,\n",
       "        9.67178047e-02,  1.76384658e-01, -3.54634225e-02,  1.23027198e-01,\n",
       "        1.27308547e-01,  7.43818954e-02, -7.57130086e-02, -1.14989795e-01,\n",
       "       -4.12835032e-02, -9.14284028e-03,  2.74244636e-01,  2.01127961e-01,\n",
       "        7.64898211e-02, -3.65237296e-01,  1.13204330e-01,  8.54145437e-02,\n",
       "       -1.00724556e-01,  1.60300419e-01,  1.62792996e-01, -2.01577187e-01,\n",
       "       -2.94318765e-01, -9.03087929e-02, -1.12690747e-01, -2.36844774e-02,\n",
       "       -4.28256869e-01, -1.17189415e-01,  2.29112849e-01, -8.28808025e-02,\n",
       "       -4.25634146e-01, -3.64379436e-01, -7.13650361e-02, -1.90144539e-01,\n",
       "        6.87066326e-03,  1.48484232e-02,  1.77946210e-01,  1.81121826e-01,\n",
       "       -2.69022007e-02, -9.54891741e-03,  2.89744269e-02, -2.42981911e-01,\n",
       "       -1.70941129e-01, -4.82342998e-03, -7.02783524e-04, -2.27740273e-01,\n",
       "       -1.72800973e-01, -1.27308769e-02,  2.30893269e-01,  5.35238646e-02,\n",
       "        2.12870557e-02, -2.04236358e-02,  1.89132676e-01, -3.94848846e-02,\n",
       "       -9.10040215e-02,  8.92355889e-02, -8.18222985e-02,  5.58757745e-02,\n",
       "       -1.68974504e-01,  8.49855766e-02, -1.11382507e-01, -2.90302545e-01,\n",
       "       -1.41914003e-02,  1.59132093e-01,  1.47715062e-01,  1.86919793e-02,\n",
       "        1.17414370e-02, -2.07834274e-01,  9.61286109e-03, -2.32022703e-02,\n",
       "       -2.77668417e-01, -1.88603811e-02,  8.91737118e-02, -1.35947511e-01,\n",
       "       -2.02856469e-03,  5.49693219e-02, -1.85341276e-02,  1.12926140e-01,\n",
       "        2.81556338e-01, -2.84257472e-01, -1.23987757e-01,  1.02725185e-01,\n",
       "       -8.62205848e-02,  6.02735057e-02, -1.84652172e-02,  1.20814472e-01,\n",
       "       -1.37859618e-03,  9.45790857e-02,  8.14568549e-02,  1.92121565e-02,\n",
       "       -8.94231070e-03, -2.14111477e-01, -3.16160232e-01, -1.86573546e-02,\n",
       "       -6.90883473e-02, -1.30010962e-01, -4.10868563e-02, -3.07481170e-01,\n",
       "        1.20066963e-01, -2.37131715e-01,  8.13767463e-02,  1.88772812e-01,\n",
       "       -6.05665632e-02, -1.24549434e-01, -2.61325985e-01,  1.30334273e-01,\n",
       "       -1.18466467e-01, -1.27582431e-01, -2.44454630e-02, -8.85202289e-02,\n",
       "        1.03244200e-01, -8.08538646e-02,  1.12906374e-01,  3.43279615e-02,\n",
       "       -8.71110782e-02, -1.14432700e-01, -1.20910279e-01,  8.83991271e-03,\n",
       "       -3.71200661e-03,  1.67150963e-02, -1.76827371e-01,  1.04882397e-01,\n",
       "        1.02952151e-02,  2.94985712e-01, -5.62091358e-02, -1.93675801e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special word \n",
    "navec['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:30:01.941380Z",
     "start_time": "2022-04-02T11:30:01.917942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special word \n",
    "navec['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/crawl-vectors.html  - Russian, bin\n",
    "https://radimrehurek.com/gensim/models/fasttext.html  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_path = datapath(\"crime-and-punishment.bin\")\n",
    "wv = load_facebook_vectors(cap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'landlord' in wv.key_to_index  # Word is out of vocabulary\n",
    "\n",
    "\n",
    "\n",
    "True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_vector = wv['landlord']  # Even OOV words have vectors in FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'landlady' in wv.key_to_index  # Word is in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_vector = wv['landlady']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RusVectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rusvectores.org/ru/models/  \n",
    "https://habr.com/ru/post/275913/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:42:34.158405Z",
     "start_time": "2022-04-02T11:42:34.098049Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:42:35.166048Z",
     "start_time": "2022-04-02T11:42:35.144026Z"
    }
   },
   "outputs": [],
   "source": [
    "def canonize_words(words: list) -> list:\n",
    "    stop_words = ('быть', 'мой', 'наш', 'ваш', 'их', 'его', 'её', 'их',\n",
    "                  'этот', 'тот', 'где', 'который', 'либо', 'нибудь', 'нет', 'да')\n",
    "    grammars = {'NOUN': '_S',\n",
    "                'VERB': '_V', 'INFN': '_V', 'GRND': '_V', 'PRTF': '_V', 'PRTS': '_V',\n",
    "                'ADJF': '_A', 'ADJS': '_A',\n",
    "                'ADVB': '_ADV',\n",
    "                'PRED': '_PRAEDIC'}\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    normalized = []\n",
    "    for i in words:\n",
    "        forms = morph.parse(i)\n",
    "        try:\n",
    "            form = max(forms, key=lambda x: (x.score, x.methods_stack[0][2]))\n",
    "        except Exception:\n",
    "            form = forms[0]\n",
    "            print(form)\n",
    "        if not (form.tag.POS in ['PREP', 'CONJ', 'PRCL', 'NPRO', 'NUMR']\n",
    "                or 'Name' in form.tag\n",
    "                or 'UNKN' in form.tag\n",
    "                or form.normal_form in stop_words):  # 'ADJF'\n",
    "            normalized.append(form.normal_form + grammars.get(form.tag.POS, ''))\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T11:43:05.485986Z",
     "start_time": "2022-04-02T11:43:04.746603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кек_S']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canonize_words(['лол', 'кек'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.load_w2v_model(sem.WORD2VEC_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_model(file_name: str) -> dict:\n",
    "    poems = read_poems(file_name)\n",
    "    bags, voc = make_bags(poems)\n",
    "    w2v_model = sem.load_w2v_model(sem.WORD2VEC_MODEL_FILE)\n",
    "    sd = [sem.semantic_density(bag, w2v_model, unknown_coef=-0.001) for bag in bags]\n",
    "    sa = [sem.semantic_association(bag, w2v_model) for bag in bags]\n",
    "    rates = [0.0 for _ in range(len(poems))]\n",
    "    return {'poems'       : poems,\n",
    "            'bags'        : bags,\n",
    "            'vocabulary'  : voc,\n",
    "            'density'     : sd,\n",
    "            'associations': sa,\n",
    "            'rates'       : rates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(bag1, bag2: list, w2v_model, unknown_coef=0.0) -> float:\n",
    "    sim_sum = 0.0\n",
    "    for i in range(len(bag1)):\n",
    "        for j in range(len(bag2)):\n",
    "            try:\n",
    "                sim_sum += w2v_model.similarity(bag1[i], bag2[j])\n",
    "            except Exception:\n",
    "                sim_sum += unknown_coef\n",
    "    return sim_sum / (len(bag1) * len(bag2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.KeyedVectors.load_word2vec_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    ">>>\n",
    "cap_path = datapath(\"crime-and-punishment.bin\")\n",
    "wv = load_facebook_vectors(cap_path)\n",
    ">>>\n",
    "'landlord' in wv.key_to_index  # Word is out of vocabulary\n",
    "False\n",
    "oov_vector = wv['landlord']  # Even OOV words have vectors in FastText\n",
    ">>>\n",
    "'landlady' in wv.key_to_index  # Word is in the vocabulary\n",
    "True\n",
    "iv_vector = wv['landlady']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existent_word = \"computer\"\n",
    "existent_word in model.wv.key_to_index\n",
    "True\n",
    "computer_vec = model.wv[existent_word]  # numpy vector of a word\n",
    ">>>\n",
    "oov_word = \"graph-out-of-vocab\"\n",
    "oov_word in model.wv.key_to_index\n",
    "False\n",
    "oov_vec = model.wv[oov_word]  # numpy vector for OOV word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_path = datapath(\"crime-and-punishment.bin\")\n",
    "fb_model = load_facebook_model(cap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sandbox]",
   "language": "python",
   "name": "conda-env-sandbox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
